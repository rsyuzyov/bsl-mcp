#!/usr/bin/env python3
"""
1C RAG + MCP —Å–µ—Ä–≤–µ—Ä. –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –≤—ã–≥—Ä—É–∑–∫—É 1–° XML, CPU-only.
Usage: python code-search.py --source ./1c-dump
"""
import argparse
import sys


def parse_args():
    parser = argparse.ArgumentParser(description="1C RAG + MCP —Å–µ—Ä–≤–µ—Ä")
    parser.add_argument("--source", required=True, help="–ö–∞—Ç–∞–ª–æ–≥ –≤—ã–≥—Ä—É–∑–∫–∏ 1–° (—Å Configuration.xml)")
    parser.add_argument("--port", type=int, default=8000, help="MCP –ø–æ—Ä—Ç (default: 8000)")
    parser.add_argument("--index", default="./1c-index", help="–ö–∞—Ç–∞–ª–æ–≥ –∏–Ω–¥–µ–∫—Å–∞ (default: ./1c-index)")

    if len(sys.argv) == 1:
        parser.print_help()
        sys.exit(0)

    return parser.parse_args()


def main():
    args = parse_args()

    # –¢—è–∂—ë–ª—ã–µ –∏–º–ø–æ—Ä—Ç—ã —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    import os
    import re
    from pathlib import Path
    from typing import Any

    import faiss
    import pymorphy3
    import uvicorn
    from fastapi import FastAPI
    from fastapi.responses import HTMLResponse
    from llama_index.core import Settings, StorageContext, VectorStoreIndex, SimpleDirectoryReader
    from llama_index.core.node_parser import SentenceSplitter
    from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    from llama_index.vector_stores.faiss import FaissVectorStore

    # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
    morph = pymorphy3.MorphAnalyzer()

    # CPU embeddings ‚Äî –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∏ 1–°
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    Settings.embed_model = HuggingFaceEmbedding(
        model_name="intfloat/multilingual-e5-small",
        query_instruction="query: ",  # e5 —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
    )
    Settings.node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=100)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º source
    if not os.path.exists(args.source):
        print(f"–û—à–∏–±–∫–∞: –∫–∞—Ç–∞–ª–æ–≥ {args.source} –Ω–µ –Ω–∞–π–¥–µ–Ω", file=sys.stderr)
        sys.exit(1)

    if not any(Path(args.source).rglob('*.xml')):
        print(f"–û—à–∏–±–∫–∞: –≤ {args.source} –Ω–µ—Ç XML —Ñ–∞–π–ª–æ–≤", file=sys.stderr)
        sys.exit(1)

    Path(args.index).mkdir(exist_ok=True)

    def build_index(source_dir: str, index_dir: str):
        print(f"–ß—Ç–µ–Ω–∏–µ XML —Ñ–∞–π–ª–æ–≤ –∏–∑ {source_dir}...")
        docs = SimpleDirectoryReader(
            source_dir,
            recursive=True,
            required_exts=[".xml", ".bsl"]
        ).load_data()
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(docs)} —Ñ–∞–π–ª–æ–≤")
        print("–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç)...")

        vector_store = FaissVectorStore(faiss.IndexFlatIP(384))  # cosine similarity
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        index = VectorStoreIndex.from_documents(docs, storage_context=storage_context, show_progress=True)
        index.storage_context.persist(index_dir)
        print(f"–ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {index_dir}")
        return index

    # –ó–∞–≥—Ä—É–∑–∫–∞/—Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
    try:
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ —Å –¥–∏—Å–∫–∞...")
        index = VectorStoreIndex.from_persist_dir(args.index)
        print("–ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω")
    except:
        index = build_index(args.source, args.index)

    retriever = index.as_retriever(similarity_top_k=10)

    # –•—Ä–∞–Ω–∏–ª–∏—â–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è keyword –ø–æ–∏—Å–∫–∞
    all_nodes = list(index.docstore.docs.values())

    def normalize(text: str) -> str:
        """–ü—Ä–∏–≤–æ–¥–∏—Ç —Å–ª–æ–≤–∞ –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ (–ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è)"""
        words = re.findall(r'[–∞-—è—ëa-z0-9]+', text.lower())
        return ' '.join(morph.parse(w)[0].normal_form for w in words)

    def hybrid_search(query: str, top_k: int = 5):
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: keyword (–ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏ —Ç–µ–∫—Å—Ç—É) + —Å–µ–º–∞–Ω—Ç–∏–∫–∞"""
        query_lower = query.lower()
        query_normalized = normalize(query)
        
        # 1. Keyword –ø–æ–∏—Å–∫ ‚Äî —Ç–æ—á–Ω–æ–µ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        keyword_results = []
        for node in all_nodes:
            file_path = node.metadata.get("file_path", "").lower()
            text = node.text.lower()
            
            # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ ‚Äî –≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
            if query_lower in file_path:
                keyword_results.append({"node": node, "score": 2.0, "match": "filename"})
            # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ —Ç–µ–∫—Å—Ç–µ
            elif query_lower in text:
                keyword_results.append({"node": node, "score": 1.5, "match": "text"})
            # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (—Å—Ç–µ–º–º–∏–Ω–≥)
            elif query_normalized in normalize(file_path):
                keyword_results.append({"node": node, "score": 1.8, "match": "filename_stem"})
            elif query_normalized in normalize(text):
                keyword_results.append({"node": node, "score": 1.3, "match": "text_stem"})
        
        # 2. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
        semantic_nodes = retriever.retrieve(query)
        semantic_results = [
            {"node": n.node, "score": n.score, "match": "semantic"}
            for n in semantic_nodes
        ]
        
        # 3. –û–±—ä–µ–¥–∏–Ω—è–µ–º: keyword —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–µ—Ä–≤—ã–º–∏
        seen_ids = set()
        combined = []
        
        for r in keyword_results + semantic_results:
            node_id = r["node"].node_id
            if node_id not in seen_ids:
                seen_ids.add(node_id)
                combined.append(r)
        
        return combined[:top_k]

    # MCP —Å–µ—Ä–≤–µ—Ä
    app = FastAPI()

    @app.get("/", response_class=HTMLResponse)
    async def index_page():
        return """<!DOCTYPE html>
<html><head><meta charset="utf-8"><title>1C Code Search</title>
<style>body{font-family:system-ui;max-width:900px;margin:40px auto;padding:0 20px}
input{width:70%;padding:10px;font-size:16px}button{padding:10px 20px;font-size:16px}
.result{border:1px solid #ddd;margin:10px 0;padding:15px;border-radius:5px}
.file{color:#666;font-size:12px}.score{color:#090;font-weight:bold}
pre{background:#f5f5f5;padding:10px;overflow-x:auto;font-size:13px}</style></head>
<body><h1>üîç 1C Code Search</h1>
<form onsubmit="search();return false"><input id="q" placeholder="–ü–æ–∏—Å–∫ –ø–æ –∫–æ–¥—É 1–°..." autofocus>
<button>–ù–∞–π—Ç–∏</button></form><div id="results"></div>
<script>async function search(){const q=document.getElementById('q').value;
const r=document.getElementById('results');r.innerHTML='–ü–æ–∏—Å–∫...';
const res=await fetch('/search?q='+encodeURIComponent(q));const data=await res.json();
r.innerHTML=data.map(d=>`<div class="result"><span class="score">${d.score.toFixed(3)}</span>
<span class="file">${d.file}</span><pre>${d.text.replace(/</g,'&lt;')}</pre></div>`).join('')||'–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ';}</script>
</body></html>"""

    @app.post("/mcp/tools/1c_search")
    async def mcp_search(request: dict) -> dict[str, Any]:
        query = request["params"]["query"]
        results = hybrid_search(query)
        formatted = [
            {"score": r["score"], "match": r["match"], "file": r["node"].metadata.get("file_path", ""), "text": r["node"].text[:500]}
            for r in results
        ]
        return {
            "content": [{"type": "text", "text": str(formatted)}],
            "isError": False
        }

    @app.get("/search")
    async def search_get(q: str) -> list[dict[str, Any]]:
        """GET /search?q=–∑–∞–ø—Ä–æ—Å ‚Äî –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –±—Ä–∞—É–∑–µ—Ä–µ"""
        results = hybrid_search(q)
        return [
            {"score": r["score"], "match": r["match"], "file": r["node"].metadata.get("file_path", ""), "text": r["node"].text[:500]}
            for r in results
        ]

    @app.get("/health")
    async def health():
        return {"status": "ok", "source": args.source}

    print(f"MCP —Å–µ—Ä–≤–µ—Ä –Ω–∞ http://0.0.0.0:{args.port}")
    print(f"Kiro: –¥–æ–±–∞–≤—å –≤ ~/.kiro/settings/mcp.json: \"1c-rag\": {{\"url\": \"http://localhost:{args.port}\"}}")

    uvicorn.run(app, host="0.0.0.0", port=args.port)


if __name__ == "__main__":
    main()
